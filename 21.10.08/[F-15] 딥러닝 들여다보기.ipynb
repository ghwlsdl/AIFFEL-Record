{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-modification",
   "metadata": {},
   "source": [
    "# 15-1. 들어가며\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "---\n",
    "\n",
    "- 딥러닝 문제 구성에 대한 기본적인 이해를 높인다.\n",
    "- Neural Network에 사용되는 용어들에 대한 이해를 높인다.\n",
    "- 딥러닝 프레임워크를 사용하지 않고, Numpy만을 이용해 딥러닝 모델과 훈련 과정을 직접 구현해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-tucson",
   "metadata": {},
   "source": [
    "# 15-2. 신경망 구성 (1) 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-elevation",
   "metadata": {},
   "source": [
    "신경망(Neural Network)이란 무엇일까요?\n",
    "\n",
    "우리 뇌에는 1000억 개에 가까운 신경계 뉴런들이 있다고 합니다.\n",
    "이 뉴런들은 서로 매우 복잡하게 얽혀 있고, 조금 물러서서 보면 하나의 거대한 그물망과 같은 형태를 이루고 있습니다.\n",
    "보통 우리는 이를 신경망이라고 부릅니다.\n",
    "\n",
    "예전부터 인류는 자연의 모습을 본떠 인공적인 물건을 만들려는 시도를 많이 해왔습니다.\n",
    "물론 이것이 자연의 모습을 본뜬 것만이 성공적이라는 뜻은 아니지만요. (새가 나는 방식과 비행기가 나는 방식은 다른 것처럼요.)\n",
    "\n",
    "머신러닝/딥러닝 과학자들도 자연에서 답을 찾으려 노력했고, 우리 뇌 속의 신경망 구조에 착안해서 퍼셉트론(Perceptron)이라는 형태를 제안하며 이를 연결한 형태를 인공신경망(Artificial Neural Network)이라고 부르기 시작했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-fortune",
   "metadata": {},
   "source": [
    "MNIST Revisited\n",
    "---\n",
    "아마도 여러분은 AIFFEL 학습 과정 중에 딥러닝 모델 학습을 몇 번 경험해 보셨을 것입니다.\n",
    "그렇다면 아마도 가장 처음 다루어본 데이터셋은 틀림없이 MNIST라는 숫자 이미지 데이터셋이었을 것입니다.\n",
    "\n",
    "딥러닝 프레임워크를 이용하면 몇 줄 안되는 코드만으로 MNIST 데이터셋을 99% 이상의 정확도로\n",
    "분류할 수 있는 이미지 분류기를 만들 수 있으며, 이를 활용해서\n",
    "다양한 카테고리의 이미지 분류기로 확장해 나갈 수 있다는 것도 이미 알고 계실 것입니다.\n",
    "\n",
    "그럼 한번 MNIST 이미지 분류기 모델이 어떻게 구성되었는지 기억을 되짚어 볼까요?\n",
    "아래는 이미 여러 번 구현해 보았던 Tensorflow 기반 분류 모델 예시 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handled-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.8114 - accuracy: 0.8171\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2443 - accuracy: 0.9323\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1863 - accuracy: 0.9479\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1568 - accuracy: 0.9545\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1324 - accuracy: 0.9625\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1183 - accuracy: 0.9666\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1056 - accuracy: 0.9705\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0928 - accuracy: 0.9749\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0845 - accuracy: 0.9772\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0750 - accuracy: 0.9795\n",
      "313/313 - 1s - loss: 0.1062 - accuracy: 0.9684\n",
      "test_loss: 0.10622765123844147 \n",
      "test_accuracy: 0.9684000015258789\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-showcase",
   "metadata": {},
   "source": [
    "이미 여러분은 Conv2D같이 이미지 분류에 특화된 모델을 사용해 본 경험이 있을 것입니다만,\n",
    "오늘은 그보다 가장 기본적인 신경망 형태인 다층 퍼셉트론(Multi-Layer Perceptron; MLP)만을 이용해서 더욱 간단하게 구현해 보았습니다.\n",
    "뇌 속의 뉴런이 1000억 개라지만 위에서 만든 모델은 굳이 Conv2D를 사용하지도 않았는데도 39,760개의 파라미터만으로 테스트 성능이 97%에 육박하고 있습니다.\n",
    "오늘은 인공신경망의 실제 구현 원리를 보다 명확하게 이해하기 위해, 그동안 들춰보지 않았던 프레임워크 내부에서 일어나는 일을 Numpy를 활용해 직접 구현해 보면서 이해해 보고자 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-fiber",
   "metadata": {},
   "source": [
    "다층 퍼셉트론 Overview\n",
    "---\n",
    "위의 이미지는 총 3개의 레이어로 구성된 퍼셉트론을 나타냅니다. 위에서 보았던 예시 코드와도 동일합니다. 은닉층에는 H개의 노드이, 출력층에는 K개의 노드가 존재하는 인공신경망을 표현한 것입니다. (+1 부분은 bias를 뜻하는 부분이므로 이전 레이어와의 연결이 없습니다. ) 위의 코드에서는 H=50, K=10, 그리고 입력층 노드 개수 d=784로 정의되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-wallet",
   "metadata": {},
   "source": [
    "위의 이미지를 보면 입력값이 있는 입력층(input layer), 최종 출력값이 있는 출력층(output layer), 그리고 그 사이에 있는 층인 은닉층(hidden layer)이 있습니다. 보통 입력층과 출력층 사이에 몇 개의 층이 존재하든 모두 은닉층이라고 부릅니다.\n",
    "\n",
    "보통 그림으로 인공신경망을 표현할 때에는 노드를 기준으로 레이어를 표시해서 3개의 레이어라고 생각할 수 있지만, 실제로는 총 2개의 레이어를 가졌습니다. 레이어 개수를 셀 때는 노드와 노드 사이의 연결하는 부분이 몇 개 존재하는지 세면 보다 쉽게 알 수 있습니다.\n",
    "\n",
    "이렇게 인공신경망이 어떻게 생겼는지 대략 알아봤는데요. 인공신경망 중에서도 위의 이미지처럼 2개 이상의 레이어를 쌓아서 만든 것을 보통 다층 퍼셉트론(Multi-Layer Perceptron; MLP)이라고 부릅니다. 그리고 입력층, 출력층을 제외한 은닉층이 많아지면 많아질수록 인공신경망이 DEEP 해졌다고 이야기합니다.\n",
    "\n",
    "여기서 좀 감이 오시나요? 우리가 지금 알아보려고 하는 딥러닝이 바로 이 인공신경망이 DEEP해졌다는 뜻에서 나온 단어입니다. 그래서 우리가 하려는 딥러닝은 충분히 깊은 인공신경망을 활용하며 이를 보통 다른 단어로 DNN(Deep Neural Network)라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-savannah",
   "metadata": {},
   "source": [
    "💡 Tips\n",
    "Fully-Connected Neural Network와 같은 단어를 들어보신 분들이 있으신가요? 이는 앞에서 설명드렸던 MLP의 다른 용어입니다. 이 Fully-Connnected Nerual Network는 서로 다른 층에 위치한 노드 간에는 연결 관계가 존재하지 않으며, 인접한 층에 위치한 노드들 간의 연결만 존재한다는 의미를 내포합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-repeat",
   "metadata": {},
   "source": [
    "Parameters/Weights\n",
    "---\n",
    "앞에서 설명한 입력층-은닉층, 은닉층-출력층 사이에는 사실 각각 행렬(Matrix)이 존재합니다. 여러분 모두 고등학교 때 배우셨던 행렬 곱셈 기억나시겠죠?\n",
    "\n",
    "예를 들어 입력값이 100개, 은닉 노드가 20개라면 사실 이 입력층-은닉층 사이에는 100x20의 형태를 가진 행렬이 존재합니다. 똑같이, MNIST 데이터처럼 10개의 클래스를 맞추는 문제를 풀기 위해 출력층이 10개의 노드를 가진다면 은닉층-출력층 사이에는 20x10의 형태를 가진 행렬이 존재하게 됩니다.\n",
    "\n",
    "이 행렬들을 Parameter 혹은 Weight라고 부릅니다. 두 단어는 보통 같은 뜻으로 사용되지만, 실제로 Parameter에는 위의 참고 자료에서 다룬 bias 노드도 포함된다는 점만 유의해 주세요.\n",
    "\n",
    "이때 인접한 레이어 사이에는 아래와 같은 관계가 성립합니다.\n",
    "\n",
    "y=W⋅X+b\n",
    "우리가 위에서 간단히 만들어 보았던 MLP 기반 딥러닝 모델을 Numpy로 다시 만들어 본다고 생각해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serial-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped 의 알 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hawaiian-framing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approximate-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.17901513e-04, -4.52699356e-02, -2.95871509e-01, -4.53762181e-01,\n",
       "       -4.82011960e-01,  8.64558099e-01,  2.61235009e+00, -5.11117771e-01,\n",
       "        6.44833934e-01,  4.17147912e-02, -1.17496331e-01,  7.12236888e-01,\n",
       "       -9.53228336e-01,  3.13564944e-02,  6.49298056e-01, -1.52467197e-01,\n",
       "        4.24391733e-01, -1.88107561e+00, -1.64095142e+00,  3.17532275e-01,\n",
       "       -2.62761625e-01, -1.06043547e-02,  5.72190408e-01, -9.74457870e-01,\n",
       "        1.10781604e+00,  1.65148431e+00,  1.04122996e-01,  7.02509955e-01,\n",
       "        1.79305062e-01,  4.93817408e-01,  1.59251498e-01, -1.69239060e+00,\n",
       "       -9.46619226e-03,  1.47023930e+00,  4.40621611e-01,  3.43771212e-01,\n",
       "       -4.30024400e-01,  2.09429416e-01,  6.95685851e-01, -9.54110933e-01,\n",
       "        5.69381041e-01,  7.15654201e-01,  5.77802216e-01,  8.52887603e-01,\n",
       "        8.94217140e-01,  4.50902122e-01, -2.22645829e-01,  1.30888992e+00,\n",
       "        1.13811530e+00,  1.72109496e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-actor",
   "metadata": {},
   "source": [
    "# 15-3. 신경망 구성 (2) 활성화 함수와 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-register",
   "metadata": {},
   "source": [
    "활성화 함수 (Activation Functions)\n",
    "---\n",
    "\n",
    "MLP의 또 다른 중요한 구성요소는 바로 활성화 함수인데요.\n",
    "\n",
    "딥러닝에서는 이 활성화 함수의 존재가 필수적입니다.\n",
    "\n",
    "수학적인 이유가 있지만, 간단히만 설명하자면 이 활성화 함수는 보통 비선형 함수를 사용하는데\n",
    "\n",
    "이 비선형 함수를 MLP 안에 포함시키면서 모델의 표현력이 좋아지게 됩니다.\n",
    "\n",
    "(정확히는 레이어 사이에 이 비선형 함수가 포함되지 않은 MLP는 한 개의 레이어로 이루어진 모델과 수학적으로 다른 점이 없습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-colombia",
   "metadata": {},
   "source": [
    "### 1. Sigmoid\n",
    "\n",
    "σ(x)=1/(1+e^−x)\n",
    "\n",
    "이전 스텝에서 우리는 은닉층의 출력에다 활성화 함수로 sigmoid를 사용한 바 있습니다.\n",
    "\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "그럼 첫 번째 은닉층의 출력 a1에다가 sigmoid를 적용해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "derived-kentucky",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49977052 0.48868445 0.42656704 0.38846665 0.38177714 0.70361209\n",
      " 0.93165219 0.37493153 0.65584536 0.51042719 0.47065966 0.67089524\n",
      " 0.27823604 0.50783848 0.65685226 0.46195687 0.60453367 0.13226538\n",
      " 0.16233564 0.57872273 0.43468496 0.49734894 0.63926845 0.27399285\n",
      " 0.75172173 0.83909156 0.52600726 0.66874403 0.54470655 0.6210053\n",
      " 0.53972895 0.15546171 0.49763347 0.81309376 0.60840714 0.5851063\n",
      " 0.3941205  0.55216682 0.66723058 0.27805883 0.63862034 0.67164933\n",
      " 0.64056154 0.70117253 0.70975967 0.6108537  0.44456734 0.78732734\n",
      " 0.75733344 0.84826982]\n"
     ]
    }
   ],
   "source": [
    "# 위 수식의 sigmoid 함수를 구현해 봅니다.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  \n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])  # sigmoid의 출력은 모든 element가 0에서 1사이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-library",
   "metadata": {},
   "source": [
    "예전부터 활성화 함수로 많이 써오던 sigmoid 함수입니다.\n",
    "\n",
    "현재는 아래에서 설명할 ReLU 함수를 더 많이 사용하는데 그 이유는 다음과 같습니다.\n",
    "\n",
    "- vanishing gradient 현상이 발생한다.\n",
    "- exp 함수 사용 시 비용이 크다.\n",
    "\n",
    "아래 참고 자료는 아래 섹션인 오차역전파법(BackPropagation)을 공부하신 다음 보시면 더욱 이해하기 편할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-england",
   "metadata": {},
   "source": [
    "### 2. Tanh\n",
    "tanh(x) =(e^x−e^−x) / (e^x + e^−x)\n",
    "\n",
    "- tanh 함수는 함수의 중심값을 0으로 옮겨 sigmoid의 최적화 과정이 느려지는 문제를 해결.\n",
    "- vanishing gradient 문제 존재."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-voltage",
   "metadata": {},
   "source": [
    "### 3. ReLU\n",
    "\n",
    "f(x)=max(0,x)\n",
    "\n",
    "- sigmoid, tanh 함수에 비해 학습이 빠름.\n",
    "- 연산 비용이 크지 않고, 구현이 매우 간단하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acoustic-investment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go~\n"
     ]
    }
   ],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache\n",
    "\n",
    "print('go~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greatest-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.23177177  0.09154804  0.6153682   0.3615112  -0.48588492  0.67354776\n",
      "  0.16215521  0.14277771  0.48480508  0.02321346]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mediterranean-underwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "negative-hearts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06231344, 0.08609893, 0.14537507, 0.11278238, 0.04833056,\n",
       "       0.15408381, 0.09239789, 0.09062468, 0.12758132, 0.08041192])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-attempt",
   "metadata": {},
   "source": [
    "손실함수 (Loss Functions)\n",
    "---\n",
    "\n",
    "이렇게 비선형 활성화 함수를 가진 여러 개의 은닉층을 거친 다음 신호 정보들은 출력층으로 전달됩니다. 이때 우리가 원하는 정답과 전달된 신호 정보들 사이의 차이를 계산하고, 이 차이를 줄이기 위해 각 파라미터들을 조정하는 것이 딥러닝의 전체적인 학습 흐름입니다.\n",
    "\n",
    "이 차이를 구하는 데 사용되는 함수는 손실함수(Loss function) 또는 비용함수(Cost function)라고 부릅니다. 대표적으로 다음과 같은 두 가지 손실함수가 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-chile",
   "metadata": {},
   "source": [
    "교차 엔트로피 (Cross Entropy)\n",
    "---\n",
    "오늘 우리는 바로 Cross Entropy를 이용해 모델을 학습할 것입니다.\n",
    "\n",
    "Cross Entropy는 두 확률분포 사이의 유사도가 클수록 작아지는 값입니다. 아직 별로 학습되지 않은 현재의 모델이 출력하는 softmax 값 \n",
    "ˆ 은 10개의 숫자 각각의 확률이 대부분 0.1 근처를 오가는 정도입니다.\n",
    "\n",
    "모델을 학습하게 되면, ˆ 이 점점 정답에 가까워지게 됩니다. 정말 그렇게 되는지 다음 스텝에서 살펴봅시다.\n",
    "\n",
    "우선은 ˆ과 정답을 비교해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "employed-maple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_one_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "arabic-editing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06231344 0.08609893 0.14537507 0.11278238 0.04833056 0.15408381\n",
      " 0.09239789 0.09062468 0.12758132 0.08041192]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "material-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.601999191092317"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-sport",
   "metadata": {},
   "source": [
    "# 15-4. 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-combine",
   "metadata": {},
   "source": [
    "그럼 이제 오차는 구했습니다. 다음 단계는 무엇일까요?\n",
    "\n",
    "앞서 말씀드렸지만 우리는 이 오차를 줄이는 것이 목표입니다. 이 상황은 우리가 마치 산꼭대기에 서 있는 것과 동일하게 생각할 수 있습니다.\n",
    "\n",
    "우리는 얼른 이 산에서 내려가서 집에 가고 싶은 사람들이죠. 여러분들이라면 어디로 내려가야 할지 모를 때 어떻게 하시겠어요?\n",
    "\n",
    "아마도 내리막길을 따라 일단 내려가 보지 않을까요?\n",
    "\n",
    "경사하강법(Gradient Descent) 또한 동일한 원리입니다. 각 단계에서의 기울기를 구해서 해당 기울기가 가리키는 방향으로 이동하는 방법이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-participation",
   "metadata": {},
   "source": [
    "위의 이미지처럼 각 시점의 기울기가 가리키는 방향으로 이동해나가는 것이죠.\n",
    "\n",
    "그럼 경사하강법을 사용하면 항상 산 아래에 잘 도착할 수 있을까요?\n",
    "\n",
    "예를 들어, 우리가 너무 크게 발걸음을 내딛을 수 있는 거인이라면 아마도 산 아래로 내려가지 못하고 또 다른 골짜기에 빠지고 말 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-seller",
   "metadata": {},
   "source": [
    "또한, 이미 생각해 보신 분들도 있겠지만,\n",
    "\n",
    "아무리 우리가 발걸음을 잘 내디딘다고 해도 어디서 출발했느냐에 따라\n",
    "\n",
    "산 아래로 내려가는 시간이 빨라질 수도 느려질 수도 있습니다.\n",
    "\n",
    "이는 parameter의 값들을 어떻게 초기화하는지의 문제와 맞닿아 있으니, 아래의 참고 자료를 확인해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-potato",
   "metadata": {},
   "source": [
    "X-Y 좌표축의 기울기란 X의 변화에 따른 Y의 변화량을 의미합니다. Y를 X로 미분해서 구하지요.\n",
    "\n",
    "우리는 파라미터 W의 변화에 따른 오차(Loss) L의 변화량을 구하려고 합니다.\n",
    "\n",
    "그러면 오차 기울기가 커지는 방향의 반대 방향으로 파라미터를 조정해 주면 됩니다.\n",
    "\n",
    "단, 조정을 너무 많이 해주면 안 되기 때문에 적절한 step size 역할을 하는 learning rate가 필수적입니다.\n",
    "\n",
    "그 과정을 Numpy를 통해 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vietnamese-gateway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01246269,  0.01721979,  0.02907501,  0.02255648,  0.00966611,\n",
       "        -0.16918324,  0.01847958,  0.01812494,  0.02551626,  0.01608238],\n",
       "       [-0.18712227,  0.0185282 ,  0.03109982,  0.0236723 ,  0.01083907,\n",
       "         0.03180446,  0.01972896,  0.01778314,  0.02169136,  0.01197495],\n",
       "       [ 0.01628565,  0.02157513,  0.02668964,  0.02456397, -0.19038879,\n",
       "         0.02838714,  0.01595393,  0.01770457,  0.02299893,  0.01622984],\n",
       "       [ 0.01552675, -0.18531629,  0.0313917 ,  0.0282854 ,  0.00907748,\n",
       "         0.02678402,  0.0147113 ,  0.02180615,  0.0229661 ,  0.01476739],\n",
       "       [ 0.01491965,  0.0177593 ,  0.02918287,  0.0250114 ,  0.01023266,\n",
       "         0.03076371,  0.0169318 ,  0.02021309,  0.02219934, -0.18721382]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brilliant-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0929675 , -0.03951976,  0.08594537,  0.07212224, -0.07742935,\n",
       "        -0.00264449,  0.05036208,  0.05570108,  0.06659457, -0.11816424],\n",
       "       [-0.10603116, -0.05011894,  0.0845461 ,  0.07047045, -0.07400747,\n",
       "        -0.03044122,  0.04968546,  0.05426786,  0.06575826, -0.06412934],\n",
       "       [-0.00917854, -0.07123295,  0.05771724,  0.04953122, -0.08546645,\n",
       "        -0.02587822,  0.03286049,  0.03804466,  0.04586996, -0.03226741],\n",
       "       [-0.02158413, -0.09040899,  0.07200594,  0.06257137, -0.13444672,\n",
       "         0.02597514,  0.04064266,  0.0477352 ,  0.05649125, -0.05898171],\n",
       "       [-0.04615216, -0.06808135,  0.06141626,  0.05122219, -0.04313373,\n",
       "        -0.06967572,  0.03564391,  0.0395894 ,  0.04858088, -0.00940968],\n",
       "       [-0.04670341, -0.0569579 ,  0.06048738,  0.05091563, -0.04942453,\n",
       "        -0.02589684,  0.03496331,  0.03935229,  0.04724666, -0.05398259],\n",
       "       [-0.07628539, -0.0170418 ,  0.05980649,  0.05007033, -0.08650724,\n",
       "        -0.0145595 ,  0.03549057,  0.03827854,  0.04693105, -0.03618305],\n",
       "       [-0.05267973, -0.03875701,  0.06084109,  0.05141592, -0.08407218,\n",
       "        -0.01515752,  0.03551959,  0.03942877,  0.04783087, -0.0443698 ],\n",
       "       [-0.0664896 , -0.06081344,  0.07796127,  0.06520078, -0.03540507,\n",
       "        -0.03367918,  0.04528112,  0.05071007,  0.06052836, -0.1032943 ],\n",
       "       [-0.05519105, -0.04987815,  0.0633156 ,  0.0518866 , -0.00932708,\n",
       "        -0.10332262,  0.03734455,  0.04049485,  0.05018533, -0.02550803],\n",
       "       [-0.10219257, -0.05116882,  0.06887318,  0.05672655, -0.03534776,\n",
       "        -0.05143689,  0.04058904,  0.04376434,  0.05348611, -0.02329318],\n",
       "       [-0.0601937 , -0.01433873,  0.06512199,  0.05405876, -0.05125594,\n",
       "        -0.05047816,  0.03871957,  0.04193735,  0.05137002, -0.07494115],\n",
       "       [-0.05256014, -0.08339018,  0.0701474 ,  0.05929232, -0.08323503,\n",
       "        -0.0402583 ,  0.04036542,  0.04547043,  0.05523779, -0.01106969],\n",
       "       [-0.07381997, -0.01366022,  0.06940526,  0.05715387, -0.02723154,\n",
       "        -0.06032291,  0.04137204,  0.04456557,  0.05441116, -0.09187326],\n",
       "       [-0.02272696, -0.05177869,  0.04055827,  0.03430104, -0.0351067 ,\n",
       "        -0.02400951,  0.02319291,  0.02648116,  0.03183435, -0.02274587],\n",
       "       [-0.03022879, -0.09245698,  0.067144  ,  0.05795467, -0.08633717,\n",
       "         0.0266068 ,  0.03774302,  0.04451675,  0.05197858, -0.07692087],\n",
       "       [-0.04814377, -0.09399431,  0.07295883,  0.06184265, -0.08508829,\n",
       "        -0.03848629,  0.04177259,  0.04745781,  0.05740076, -0.01571998],\n",
       "       [-0.04220602, -0.08513122,  0.08115299,  0.06863488, -0.08492402,\n",
       "        -0.05138777,  0.04675746,  0.05293966,  0.06407517, -0.04991114],\n",
       "       [-0.01240444, -0.08277212,  0.05771084,  0.04956978, -0.05582434,\n",
       "        -0.00365166,  0.03244551,  0.03833072,  0.04500166, -0.06840595],\n",
       "       [-0.08130431, -0.04524612,  0.0799685 ,  0.06769467, -0.12508985,\n",
       "        -0.00209179,  0.04677224,  0.05171282,  0.06266773, -0.05508389],\n",
       "       [-0.0791    , -0.02615765,  0.05377343,  0.04532662, -0.08542385,\n",
       "         0.01429619,  0.0316052 ,  0.03446857,  0.0416719 , -0.03046041],\n",
       "       [-0.05178461, -0.04056614,  0.0717842 ,  0.0604326 , -0.05496641,\n",
       "        -0.01560532,  0.04180545,  0.04691443,  0.05593627, -0.11395047],\n",
       "       [-0.09037984, -0.02096221,  0.07072467,  0.05991773, -0.1300878 ,\n",
       "         0.02291007,  0.04169177,  0.04554377,  0.05517495, -0.0545331 ],\n",
       "       [-0.10242076, -0.05724044,  0.07642968,  0.06354303, -0.02056545,\n",
       "        -0.0069333 ,  0.04450498,  0.04934022,  0.05835427, -0.10501223],\n",
       "       [-0.04111823, -0.04151267,  0.07024664,  0.05948606, -0.09423431,\n",
       "        -0.02778642,  0.04098011,  0.04579617,  0.05557233, -0.06742969],\n",
       "       [-0.06961   , -0.00299984,  0.07407814,  0.06159707, -0.06127145,\n",
       "        -0.04120662,  0.04416223,  0.04782193,  0.05825362, -0.11082509],\n",
       "       [-0.03704261, -0.12392691,  0.07153388,  0.06133882, -0.06534792,\n",
       "         0.00359981,  0.03995246,  0.04722266,  0.05527037, -0.05260056],\n",
       "       [-0.05779124,  0.00317808,  0.05272183,  0.04405848, -0.08079047,\n",
       "        -0.02383594,  0.03162198,  0.03377884,  0.04180491, -0.04474648],\n",
       "       [-0.1369968 , -0.02458475,  0.09110959,  0.07511503, -0.07066482,\n",
       "        -0.05063382,  0.0543681 ,  0.05790272,  0.07097574, -0.06659099],\n",
       "       [-0.02570468, -0.03929341,  0.04036403,  0.03415175, -0.04285757,\n",
       "        -0.01705407,  0.02328175,  0.02632327,  0.03170209, -0.03091317],\n",
       "       [-0.05560436, -0.11202871,  0.09974505,  0.08439954, -0.09468784,\n",
       "        -0.05249794,  0.05726704,  0.0651496 ,  0.0783698 , -0.07011218],\n",
       "       [-0.1183255 , -0.08535912,  0.09842828,  0.08216852, -0.06599977,\n",
       "        -0.03277286,  0.05726907,  0.06340172,  0.076155  , -0.07496533],\n",
       "       [-0.07853355, -0.02748739,  0.06973738,  0.0578188 , -0.03377068,\n",
       "        -0.03460585,  0.04117139,  0.04493736,  0.05419898, -0.09346644],\n",
       "       [ 0.00351643, -0.02541245,  0.05121107,  0.04399548, -0.09598464,\n",
       "        -0.02517536,  0.02977587,  0.03383948,  0.04131068, -0.05707656],\n",
       "       [-0.07989337, -0.03467583,  0.0861457 ,  0.07219441, -0.0896719 ,\n",
       "        -0.03324387,  0.05072393,  0.05572159,  0.06758781, -0.09488847],\n",
       "       [-0.06241206, -0.10062878,  0.10266354,  0.0873618 , -0.11413406,\n",
       "        -0.00852681,  0.05891083,  0.06732872,  0.0801298 , -0.11069299],\n",
       "       [-0.10521305, -0.02968278,  0.07992798,  0.06632426, -0.09252819,\n",
       "        -0.05353328,  0.04756411,  0.05086207,  0.06293048, -0.02665159],\n",
       "       [-0.11771531, -0.03911107,  0.10500726,  0.08699149, -0.07258608,\n",
       "        -0.07303147,  0.06223994,  0.06737667,  0.08227309, -0.10144451],\n",
       "       [-0.07274293, -0.06165746,  0.08096756,  0.06821399, -0.05169447,\n",
       "         0.00248055,  0.04681436,  0.05290316,  0.06239119, -0.12767594],\n",
       "       [-0.10807668, -0.0524298 ,  0.08208404,  0.06786445, -0.06017009,\n",
       "        -0.06436211,  0.04847038,  0.05230123,  0.06421447, -0.0298959 ],\n",
       "       [-0.00802218, -0.07218263,  0.08176714,  0.07090516, -0.13154188,\n",
       "         0.0298348 ,  0.04650143,  0.05458035,  0.0641797 , -0.1360219 ],\n",
       "       [-0.10470136, -0.04513413,  0.0839254 ,  0.06998632, -0.09317834,\n",
       "        -0.03950686,  0.04951025,  0.05370313,  0.06572458, -0.04032899],\n",
       "       [-0.09067027, -0.06172739,  0.08579235,  0.07211359, -0.07008964,\n",
       "        -0.00334754,  0.04985793,  0.05567976,  0.06629991, -0.10390869],\n",
       "       [-0.09748338, -0.03535324,  0.06734012,  0.05492346, -0.00937749,\n",
       "        -0.07252017,  0.04004833,  0.04271057,  0.05244051, -0.04272871],\n",
       "       [-0.11951707, -0.03356445,  0.08006871,  0.06653941, -0.0721395 ,\n",
       "        -0.01498727,  0.04734821,  0.05115935,  0.06195067, -0.06685807],\n",
       "       [-0.02486626, -0.03564005,  0.05984243,  0.05131539, -0.08742573,\n",
       "         0.01301281,  0.0345425 ,  0.03953257,  0.04690665, -0.09722031],\n",
       "       [-0.04588931, -0.01143672,  0.04898873,  0.04153991, -0.10553977,\n",
       "        -0.01121154,  0.02904102,  0.03152248,  0.03906451, -0.01607932],\n",
       "       [-0.0825426 , -0.05751465,  0.08690797,  0.07351338, -0.10703427,\n",
       "         0.00149788,  0.05054946,  0.05647068,  0.06767245, -0.0895203 ],\n",
       "       [-0.04052688, -0.11219341,  0.09917011,  0.08513576, -0.14911322,\n",
       "        -0.00624494,  0.05654307,  0.06527125,  0.0780007 , -0.07604245],\n",
       "       [-0.07150745, -0.09600365,  0.08623415,  0.07251682, -0.08428257,\n",
       "        -0.05578961,  0.04980717,  0.05577911,  0.06773985, -0.02449383]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mechanical-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prescription-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "occupied-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "quantitative-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터를 업데이트하는 함수를 생각해 봅시다. learning_rate도 고려해야 합니다.\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-outdoors",
   "metadata": {},
   "source": [
    "# 15-5. 오차역전파법이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-frequency",
   "metadata": {},
   "source": [
    "이제 우리는 손실 함수를 통해 구해진 오차를 가지고 각 파라미터들을 조정하는 경사하강법에 대해 알게 되었습니다.\n",
    "그럼 이 기울기를 어떻게 입력층까지 전달하며 파라미터들을 조정해 나갈 수 있을까요?\n",
    "이 과정에서 쓰이는 개념이 오차역전파법(Backpropagation) 입니다.\n",
    "\n",
    "오차역전파법은 앞에서 설명한 MLP를 학습시키기 위한 일반적인 알고리즘 중 하나입니다.\n",
    "이는 출력층의 결과와 내가 뽑고자 하는 target 값과의 차이를 구한 뒤,\n",
    "그 오차 값을 각 레이어들을 지나며 역전파 해가며 각 노드가 가지고 있는 변수들을 갱신해 나가는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "requested-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "forbidden-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07246494 0.12004428 0.08770718 0.04257361 0.11161916 0.126041\n",
      "  0.1137354  0.10937236 0.0498207  0.16662138]\n",
      " [0.07086715 0.10669314 0.09749037 0.04600193 0.10587795 0.14316574\n",
      "  0.11453214 0.09351482 0.05755656 0.16430021]\n",
      " [0.10014421 0.10256104 0.08095751 0.03723466 0.12600765 0.15067429\n",
      "  0.11218652 0.0934174  0.05627014 0.14054658]\n",
      " [0.08526375 0.10697223 0.09826323 0.04765958 0.09995751 0.14882398\n",
      "  0.10453098 0.09611919 0.05911729 0.15329225]\n",
      " [0.07615192 0.12911812 0.0815203  0.03663428 0.12360692 0.14895602\n",
      "  0.10019515 0.09337859 0.06406359 0.1463751 ]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.1892555495572745\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_one_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-seventh",
   "metadata": {},
   "source": [
    "# 15-6. 모델 학습 Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-miniature",
   "metadata": {},
   "source": [
    "방금 우리는 한 스텝의 Forward Propagation과 Backward Propagation을 통해\n",
    "학습해야 할 파라미터 W1, b1, W2, b2가 업데이트되는 과정을 확인했습니다.\n",
    "\n",
    "과연 이렇게 파라미터가 업데이트될 때, 우리의 모델은 점점 더 정확한 추론을 하게 되는 것일까요?\n",
    "\n",
    "업데이트되는 과정을 다섯 스텝만 반복해 보면서 그 효과를 확인해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rental-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_one_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "modern-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.0846924  0.08533532 0.11513791 0.09356194 0.09681287 0.20079756\n",
      "  0.07018293 0.08158424 0.07120184 0.10069299]\n",
      " [0.08406563 0.08311891 0.12384193 0.12453734 0.10431409 0.18133194\n",
      "  0.07562503 0.07289064 0.07506469 0.07520979]\n",
      " [0.08493469 0.08751237 0.13286129 0.09183562 0.0922762  0.18406059\n",
      "  0.06775557 0.08355943 0.08455264 0.0906516 ]\n",
      " [0.09117419 0.07868634 0.10922759 0.10864437 0.09219516 0.17450047\n",
      "  0.09891504 0.08113096 0.07031011 0.09521575]\n",
      " [0.08259043 0.06570087 0.12243386 0.10695561 0.08601844 0.18209482\n",
      "  0.08403936 0.07815568 0.0873768  0.10463413]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.252831150142609\n",
      "---------\n",
      "[[0.09936978 0.09790954 0.09459163 0.0794705  0.11091956 0.20971361\n",
      "  0.06135185 0.07171446 0.06253333 0.11242574]\n",
      " [0.10556249 0.09781665 0.1031237  0.1061588  0.12085623 0.18431969\n",
      "  0.06643276 0.06418047 0.06642325 0.08512596]\n",
      " [0.0997924  0.10161465 0.11061771 0.07828112 0.1134071  0.18638948\n",
      "  0.05935262 0.07398466 0.07466019 0.10190007]\n",
      " [0.10704404 0.09648859 0.09182539 0.09296763 0.1056501  0.17561186\n",
      "  0.08737445 0.07259352 0.06280671 0.1076377 ]\n",
      " [0.09591732 0.07637829 0.10399367 0.09281626 0.09864491 0.18499883\n",
      "  0.07541308 0.07089385 0.07825663 0.12268716]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.0847367955690608\n",
      "---------\n",
      "[[0.11264689 0.10880005 0.07898309 0.06812403 0.12268396 0.21539511\n",
      "  0.0537181  0.06293871 0.05493608 0.12177398]\n",
      " [0.12723358 0.11081765 0.08705102 0.09107066 0.13435198 0.18347679\n",
      "  0.05828552 0.05621173 0.0585766  0.09292447]\n",
      " [0.11286849 0.11385823 0.09337936 0.06717511 0.13399048 0.18508029\n",
      "  0.05191232 0.06527284 0.06576822 0.11069466]\n",
      " [0.12115388 0.11429909 0.07831267 0.08016785 0.11668799 0.17361619\n",
      "  0.07718411 0.06478501 0.0560273  0.11776591]\n",
      " [0.10769563 0.08604309 0.08956397 0.0811246  0.10931634 0.18487019\n",
      "  0.06765867 0.06411255 0.07003541 0.13957955]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.9490111274748425\n",
      "---------\n",
      "[[0.1242355  0.11793847 0.06689438 0.05897894 0.13209036 0.21972084\n",
      "  0.04725833 0.05537477 0.04844638 0.12906203]\n",
      " [0.1484386  0.12190534 0.0743699  0.07873006 0.14473854 0.18082926\n",
      "  0.05126649 0.04920724 0.05170259 0.09881199]\n",
      " [0.12385862 0.12408773 0.07980664 0.05810161 0.15358594 0.1820903\n",
      "  0.04550784 0.05763837 0.0580294  0.11729356]\n",
      " [0.13317039 0.13177758 0.06762364 0.06971222 0.12528769 0.17025419\n",
      "  0.06840097 0.05788962 0.05008247 0.12580122]\n",
      " [0.11768172 0.0945655  0.07804094 0.07143815 0.11797151 0.18330339\n",
      "  0.06085688 0.05800715 0.06280657 0.15532819]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.8370662313927753\n",
      "---------\n",
      "[[0.13403241 0.12538569 0.05738513 0.05157341 0.13934123 0.2238404\n",
      "  0.04184117 0.04895248 0.04296856 0.13467951]\n",
      " [0.16876353 0.1310755  0.06424145 0.06863439 0.15229785 0.17758998\n",
      "  0.04530627 0.04318268 0.0457962  0.10311215]\n",
      " [0.13271515 0.13235073 0.06898813 0.05067943 0.17205176 0.17857504\n",
      "  0.04007306 0.05107839 0.05140707 0.12208125]\n",
      " [0.14301578 0.14873457 0.05904908 0.06114238 0.13167595 0.1665259\n",
      "  0.06092    0.05191122 0.04495046 0.13207466]\n",
      " [0.12581865 0.10192417 0.06870054 0.06338049 0.1247465  0.181227\n",
      "  0.0549628  0.05261523 0.0565307  0.1700939 ]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.74260705256041\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-intelligence",
   "metadata": {},
   "source": [
    "모델이 추론한 확률값 y_hat이 정답의 One-hot 인코딩 t값에 조금씩 근접하는 것과, Loss가 점점 감소하는 것이 확인 할 수 있습니다.\n",
    "\n",
    "그렇다면 우리는 경사하강법을 통해 조금씩 파라미터를 제대로 업데이트해 가고 있다고 볼 수 있겠습니다.\n",
    "\n",
    "이제 거의 근접했습니다. 몇 가지만 보완하면 우리는 모델 훈련 전과정을 완성할 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-growing",
   "metadata": {},
   "source": [
    "# 15-7. 추론 과정 구현과 정확도(Accuracy) 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "brutal-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "proof-disclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14207857, 0.13127779, 0.04980096, 0.04553359, 0.14474284,\n",
       "       0.22837847, 0.03730645, 0.04353041, 0.0383624 , 0.13898851])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "convenient-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "qualified-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14207857 0.13127779 0.04980096 0.04553359 0.14474284 0.22837847\n",
      " 0.03730645 0.04353041 0.0383624  0.13898851]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.07\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_one_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-desert",
   "metadata": {},
   "source": [
    "# 15-8. 전체 학습 사이클 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "satellite-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킬 파라미터를 초기화하는 함수\n",
    "\n",
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "\n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros(output_size)\n",
    "\n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(W2.shape)\n",
    "    print(b2.shape)\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acceptable-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(50, 10)\n",
      "(10,)\n",
      "Loss:  2.3001354852651215\n",
      "train acc, test acc | 0.09915, 0.1009\n",
      "Loss:  0.946587491930769\n",
      "train acc, test acc | 0.7937666666666666, 0.7986\n",
      "Loss:  0.3771461619708277\n",
      "train acc, test acc | 0.8759666666666667, 0.8792\n",
      "Loss:  0.4106922826309253\n",
      "train acc, test acc | 0.8973, 0.899\n",
      "Loss:  0.2039646529435182\n",
      "train acc, test acc | 0.9073166666666667, 0.9083\n",
      "Loss:  0.28741899473476507\n",
      "train acc, test acc | 0.9142333333333333, 0.916\n",
      "Loss:  0.26575039579517584\n",
      "train acc, test acc | 0.9196166666666666, 0.9225\n",
      "Loss:  0.2697154930168447\n",
      "train acc, test acc | 0.9236, 0.9266\n",
      "Loss:  0.2268982624952903\n",
      "train acc, test acc | 0.9270333333333334, 0.9283\n",
      "Loss:  0.19364458061070117\n",
      "train acc, test acc | 0.9316333333333333, 0.9327\n",
      "Loss:  0.3146160891237993\n",
      "train acc, test acc | 0.9341, 0.9343\n",
      "Loss:  0.34017498890883063\n",
      "train acc, test acc | 0.9365, 0.9369\n",
      "Loss:  0.23582162037984158\n",
      "train acc, test acc | 0.9394333333333333, 0.9382\n",
      "Loss:  0.1752358888570047\n",
      "train acc, test acc | 0.94135, 0.9413\n",
      "Loss:  0.20617279163378505\n",
      "train acc, test acc | 0.9436333333333333, 0.9423\n",
      "Loss:  0.07671210455580235\n",
      "train acc, test acc | 0.946, 0.9425\n",
      "Loss:  0.29180131394985603\n",
      "train acc, test acc | 0.9478833333333333, 0.9454\n",
      "Loss:  0.136640155044781\n",
      "train acc, test acc | 0.9493166666666667, 0.9467\n",
      "Loss:  0.15431650452392007\n",
      "train acc, test acc | 0.9509833333333333, 0.9483\n",
      "Loss:  0.1526063642034562\n",
      "train acc, test acc | 0.9516666666666667, 0.9481\n",
      "Loss:  0.12810307299265522\n",
      "train acc, test acc | 0.9536666666666667, 0.95\n",
      "Loss:  0.09245884734998032\n",
      "train acc, test acc | 0.9544666666666667, 0.9508\n",
      "Loss:  0.07941684899090327\n",
      "train acc, test acc | 0.95525, 0.9517\n",
      "Loss:  0.09163298091423636\n",
      "train acc, test acc | 0.95685, 0.9523\n",
      "Loss:  0.14985407444667845\n",
      "train acc, test acc | 0.9573166666666667, 0.9541\n",
      "Loss:  0.10556469363969433\n",
      "train acc, test acc | 0.9590833333333333, 0.9553\n",
      "Loss:  0.13553812610144197\n",
      "train acc, test acc | 0.9596, 0.9558\n",
      "Loss:  0.18194656316795746\n",
      "train acc, test acc | 0.96055, 0.9563\n",
      "Loss:  0.12053817452716373\n",
      "train acc, test acc | 0.9616, 0.957\n",
      "Loss:  0.16656636578116676\n",
      "train acc, test acc | 0.9624666666666667, 0.9573\n",
      "Loss:  0.04465701084368882\n",
      "train acc, test acc | 0.9632166666666667, 0.9581\n",
      "Loss:  0.1346939498624566\n",
      "train acc, test acc | 0.96395, 0.9596\n",
      "Loss:  0.24902343592742887\n",
      "train acc, test acc | 0.96525, 0.959\n",
      "Loss:  0.11650522618526178\n",
      "train acc, test acc | 0.96635, 0.9602\n",
      "Loss:  0.13883468364443885\n",
      "train acc, test acc | 0.9662, 0.9604\n",
      "Loss:  0.09541473905944377\n",
      "train acc, test acc | 0.9672666666666667, 0.9605\n",
      "Loss:  0.08785080634923659\n",
      "train acc, test acc | 0.9678, 0.9614\n",
      "Loss:  0.12245998261999111\n",
      "train acc, test acc | 0.9678, 0.962\n",
      "Loss:  0.1318287994168516\n",
      "train acc, test acc | 0.9691666666666666, 0.9625\n",
      "Loss:  0.12807466022129568\n",
      "train acc, test acc | 0.9697666666666667, 0.9621\n",
      "Loss:  0.10354207254200917\n",
      "train acc, test acc | 0.9704666666666667, 0.963\n",
      "Loss:  0.04854619595105966\n",
      "train acc, test acc | 0.9703166666666667, 0.9638\n",
      "Loss:  0.07831483843046433\n",
      "train acc, test acc | 0.9707166666666667, 0.9634\n",
      "Loss:  0.10128434036663325\n",
      "train acc, test acc | 0.9716166666666667, 0.9637\n",
      "Loss:  0.07842342735488621\n",
      "train acc, test acc | 0.97215, 0.9642\n",
      "Loss:  0.03274483965497641\n",
      "train acc, test acc | 0.9728166666666667, 0.9651\n",
      "Loss:  0.10711773431186035\n",
      "train acc, test acc | 0.9734333333333334, 0.9647\n",
      "Loss:  0.08809934915571416\n",
      "train acc, test acc | 0.9731166666666666, 0.9651\n",
      "Loss:  0.07536994159923124\n",
      "train acc, test acc | 0.9737833333333333, 0.9658\n",
      "Loss:  0.07325258344721272\n",
      "train acc, test acc | 0.9743333333333334, 0.9658\n",
      "Loss:  0.0447632605204715\n",
      "train acc, test acc | 0.9747666666666667, 0.9666\n",
      "Loss:  0.06710650988918168\n",
      "train acc, test acc | 0.9747666666666667, 0.9668\n",
      "Loss:  0.10423760759383038\n",
      "train acc, test acc | 0.97465, 0.9659\n",
      "Loss:  0.0865850636796794\n",
      "train acc, test acc | 0.9753833333333334, 0.9671\n",
      "Loss:  0.08161562957402024\n",
      "train acc, test acc | 0.9759, 0.9673\n",
      "Loss:  0.09648539197654578\n",
      "train acc, test acc | 0.9757, 0.9675\n",
      "Loss:  0.11998444357329645\n",
      "train acc, test acc | 0.9763666666666667, 0.9677\n",
      "Loss:  0.06175225456166414\n",
      "train acc, test acc | 0.9765333333333334, 0.9673\n",
      "Loss:  0.10737837685426405\n",
      "train acc, test acc | 0.9766166666666667, 0.968\n",
      "Loss:  0.11338036842321506\n",
      "train acc, test acc | 0.9769166666666667, 0.9681\n",
      "Loss:  0.045606017417843404\n",
      "train acc, test acc | 0.97735, 0.9676\n",
      "Loss:  0.03604645035283784\n",
      "train acc, test acc | 0.9777166666666667, 0.9679\n",
      "Loss:  0.04154272961160779\n",
      "train acc, test acc | 0.9778333333333333, 0.9679\n",
      "Loss:  0.08320256912841463\n",
      "train acc, test acc | 0.9782166666666666, 0.9694\n",
      "Loss:  0.060262537275086986\n",
      "train acc, test acc | 0.9785666666666667, 0.9682\n",
      "Loss:  0.1047183932970339\n",
      "train acc, test acc | 0.97865, 0.9686\n",
      "Loss:  0.05908289849478444\n",
      "train acc, test acc | 0.9786166666666667, 0.9691\n",
      "Loss:  0.0844088010423834\n",
      "train acc, test acc | 0.9793, 0.9695\n",
      "Loss:  0.06803787580154033\n",
      "train acc, test acc | 0.9789333333333333, 0.9696\n",
      "Loss:  0.05505359469763789\n",
      "train acc, test acc | 0.9796166666666667, 0.9688\n",
      "Loss:  0.06214425146147338\n",
      "train acc, test acc | 0.97965, 0.969\n",
      "Loss:  0.06522486906256321\n",
      "train acc, test acc | 0.9800333333333333, 0.9687\n",
      "Loss:  0.10352262810986661\n",
      "train acc, test acc | 0.98035, 0.9686\n",
      "Loss:  0.08832849450667796\n",
      "train acc, test acc | 0.9803166666666666, 0.9697\n",
      "Loss:  0.08249303993363317\n",
      "train acc, test acc | 0.9803, 0.9694\n",
      "Loss:  0.046930535135644706\n",
      "train acc, test acc | 0.9809166666666667, 0.9701\n",
      "Loss:  0.07725390698855508\n",
      "train acc, test acc | 0.9810166666666666, 0.9696\n",
      "Loss:  0.13413168578426757\n",
      "train acc, test acc | 0.9817, 0.9695\n",
      "Loss:  0.0558246797246266\n",
      "train acc, test acc | 0.9815833333333334, 0.9691\n",
      "Loss:  0.06230008916614196\n",
      "train acc, test acc | 0.9821333333333333, 0.9697\n",
      "Loss:  0.061650653051395936\n",
      "train acc, test acc | 0.9817666666666667, 0.97\n",
      "Loss:  0.032598228689600994\n",
      "train acc, test acc | 0.9819166666666667, 0.9702\n",
      "Loss:  0.033714397144865134\n",
      "train acc, test acc | 0.9822, 0.9707\n",
      "Loss:  0.14738433655903735\n",
      "train acc, test acc | 0.9825666666666667, 0.9699\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-queen",
   "metadata": {},
   "source": [
    "딥러닝 프레임워크가 없이도 Numpy만으로도 딥러닝이 가능하다는 것을 확인했습니다.\n",
    "\n",
    "물론 아쉽게도 Numpy는 GPU를 지원하지 않으므로, 방금의 딥러닝은 CPU만 사용한 것입니다.\n",
    "\n",
    "하지만 이로써 우리는 딥러닝 프레임워크 안에서 일어나는 일의 대강에 대해서 추론해 볼 수 있게 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bigger-knight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 위 훈련 과정의 Accuracy, Loss 변화를 시각화\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6 \n",
    "\n",
    "# Accuracy 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "broke-marijuana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 그리기\n",
    "x = np.arange(len(train_loss_list))\n",
    "plt.plot(x, train_loss_list, label='train acc')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0, 3.0)\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-better",
   "metadata": {},
   "source": [
    "지금까지 그동안 막연히 개념적으로만 이해해 왔던,\n",
    "\n",
    "텐서플로우 같은 딥러닝 프레임워크에서 자동으로 수행되기 때문에 구체적인 동작 메커니즘이\n",
    "\n",
    "잘 와닿지 않았던 딥러닝이라는 것을 좀 더 세부적으로 들여다보았습니다.\n",
    "\n",
    "경사하강법과 오차역전파법을 통해 파라미터가 어떻게 최적화되는지를 이해하는데\n",
    "\n",
    "오늘의 내용이 도움이 되었기를 바랍니다. 수고하셨습니다!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
