{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 인공지능과 가위바위보 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1. 인공지능과 가위바위보 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 기술은 \"데이터 준비 → 딥러닝 네트워크 설계 → 학습 → 테스트(평가)\"의\n",
    "\n",
    "# 순서대로 만들게 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2. 데이터를 준비하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sitting-gospel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)   # Tensorflow의 버전을 출력\n",
    "\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "print(len(x_train))  # x_train 배열의 크기를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "republican-failure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[1],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mounted-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001 번째 이미지의 숫자는 바로  3 입니다.\n"
     ]
    }
   ],
   "source": [
    "# index에 0에서 59999 사이 숫자를 지정해 보세요.\n",
    "index=10000     \n",
    "plt.imshow(x_train[index],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print( (index+1), '번째 이미지의 숫자는 바로 ',  y_train[index], '입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quantitative-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monetary-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "moved-tractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값: 0  최대값: 255\n"
     ]
    }
   ],
   "source": [
    "print('최소값:',np.min(x_train), ' 최대값:',np.max(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "treated-parking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소값: 0.0  최대값: 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "print('최소값:',np.min(x_train_norm), ' 최대값:',np.max(x_train_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3. 딥러닝 네트워크 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weighted-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model에 추가된 Layer 개수:  7\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "print('Model에 추가된 Layer 개수: ', len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upper-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 30,762\n",
      "Trainable params: 30,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4. 딥러닝 네트워크 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chief-astronomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Reshape - x_train_norm shape: (60000, 28, 28)\n",
      "Before Reshape - x_test_norm shape: (10000, 28, 28)\n",
      "After Reshape - x_train_reshaped shape: (60000, 28, 28, 1)\n",
      "After Reshape - x_test_reshaped shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Reshape - x_train_norm shape: {}\".format(x_train_norm.shape))\n",
    "print(\"Before Reshape - x_test_norm shape: {}\".format(x_test_norm.shape))\n",
    "\n",
    "x_train_reshaped=x_train_norm.reshape( -1, 28, 28, 1)  # 데이터갯수에 -1을 쓰면 reshape시 자동계산됩니다.\n",
    "x_test_reshaped=x_test_norm.reshape( -1, 28, 28, 1)\n",
    "\n",
    "print(\"After Reshape - x_train_reshaped shape: {}\".format(x_train_reshaped.shape))\n",
    "print(\"After Reshape - x_test_reshaped shape: {}\".format(x_test_reshaped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "seeing-password",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 5ms/step - loss: 0.4313 - accuracy: 0.8688\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0721 - accuracy: 0.9790\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0471 - accuracy: 0.9852\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0376 - accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0290 - accuracy: 0.9911\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0269 - accuracy: 0.9915\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0196 - accuracy: 0.9938\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0164 - accuracy: 0.9940\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0117 - accuracy: 0.9961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f60e28d5bd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-5. 얼마나 잘 만들었는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "precious-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 0.0377 - accuracy: 0.9893\n",
      "test_loss: 0.03768241032958031 \n",
      "test_accuracy: 0.989300012588501\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-7. 미니 프로젝트 : 가위바위보 분류기를 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "flush-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL 라이브러리 import 완료!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "\n",
    "print(\"PIL 라이브러리 import 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "charitable-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111  images to be resized.\n",
      "111  images resized.\n",
      "가위 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"가위 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "worth-colors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102  images to be resized.\n",
      "102  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hydraulic-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102  images to be resized.\n",
      "102  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "    \n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "# 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "\n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "together-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 31,050\n",
      "Trainable params: 31,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.8714 - accuracy: 0.2800\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.2594 - accuracy: 0.3318\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0834 - accuracy: 0.3864\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9954 - accuracy: 0.5734\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9170 - accuracy: 0.7342\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8314 - accuracy: 0.6409\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7095 - accuracy: 0.7830\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.6572 - accuracy: 0.6798\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5249 - accuracy: 0.7942\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.9317\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3019 - accuracy: 0.9611\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.9040\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.9576\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9462\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9551\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9653\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9655\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9728\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1197 - accuracy: 0.9666\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.9820\n",
      "10/10 - 0s - loss: 0.0814 - accuracy: 0.9841\n",
      "train_loss: 0.08143683522939682 \n",
      "train_accuracy: 0.9840764403343201\n"
     ]
    }
   ],
   "source": [
    "# 바꿔 볼 수 있는 하이퍼파라미터들\n",
    "n_channel_1=16\n",
    "n_channel_2=32\n",
    "n_dense=32\n",
    "n_train_epoch=20\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)\n",
    "\n",
    "# 모델 시험\n",
    "train_loss, train_accuracy = model.evaluate(x_train_norm, y_train, verbose=2)\n",
    "print(\"train_loss: {} \".format(train_loss))\n",
    "print(\"train_accuracy: {}\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bacterial-alpha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 70,954\n",
      "Trainable params: 70,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 6s 319ms/step - loss: 2.1798 - accuracy: 0.2613\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.2539 - accuracy: 0.4106\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.0723 - accuracy: 0.4104\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9727 - accuracy: 0.4852\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.8727 - accuracy: 0.6950\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7550 - accuracy: 0.7039\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6271 - accuracy: 0.7992\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5019 - accuracy: 0.9459\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4466 - accuracy: 0.8742\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3525 - accuracy: 0.9508\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9824\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2616 - accuracy: 0.9429\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2628 - accuracy: 0.9036\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2140 - accuracy: 0.9294\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9841\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1572 - accuracy: 0.9693\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1659 - accuracy: 0.9557\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9640\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0908 - accuracy: 0.9849\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6131020450>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_dense=32\n",
    "n_train_epoch=20\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_norm, y_train, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "challenging-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                51232     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 70,954\n",
      "Trainable params: 70,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2.0013 - accuracy: 0.2277\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.2478 - accuracy: 0.3951\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0417 - accuracy: 0.6694\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0040 - accuracy: 0.4412\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9667 - accuracy: 0.5257\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9382 - accuracy: 0.5095\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7866 - accuracy: 0.7931\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6552 - accuracy: 0.8887\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4628 - accuracy: 0.9274\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2858 - accuracy: 0.9631\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.9760\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1281 - accuracy: 0.9795\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9855\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.9444\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1022 - accuracy: 0.9763\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.9847\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9945\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9791\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9915\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f61310325d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# #from tensorflow import keras\n",
    "# #import numpy as np\n",
    "\n",
    "# #n_channel_1=32\n",
    "# #n_channel_2=64\n",
    "# #n_dense=32\n",
    "# #n_train_epoch=20\n",
    "\n",
    "# #model=keras.models.Sequential()\n",
    "# #model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "# #model.add(keras.layers.MaxPool2D(2,2))\n",
    "# #model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "# #model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "# #model.add(keras.layers.Flatten())\n",
    "# #model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "# #model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# #model.summary()\n",
    "# #model.compile(optimizer='adam',\n",
    "#                  loss='sparse_categorical_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train_norm, y_train, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "associate-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_test)의 이미지 개수는 301 입니다.\n",
      "x_test shape: (301, 28, 28, 3)\n",
      "y_test shape: (301,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(img_path, number_of_data=301):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_test)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/test\"\n",
    "(x_test, y_test)=load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0   # 입력은 0~1 사이의 값으로 정규화\n",
    "\n",
    "print(\"x_test shape: {}\".format(x_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-bidder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "matched-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 225,610\n",
      "Trainable params: 225,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.8822 - accuracy: 0.2563\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.1493 - accuracy: 0.3287\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1.0585 - accuracy: 0.4300\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9872 - accuracy: 0.7559\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.8656 - accuracy: 0.8358\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.8585\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4858 - accuracy: 0.8635\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3961 - accuracy: 0.8387\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.8783\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8765\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3024 - accuracy: 0.8594\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2793 - accuracy: 0.8700\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2567 - accuracy: 0.8843\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2310 - accuracy: 0.8890\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2029 - accuracy: 0.9081\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1767 - accuracy: 0.9243\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1667 - accuracy: 0.9245\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1454 - accuracy: 0.9529\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1258 - accuracy: 0.9508\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6130b59e90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channel_1=32\n",
    "n_channel_2=64\n",
    "n_dense=128\n",
    "n_test_epoch=20\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_test_norm, y_test, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "patent-speed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 10000\n  y sizes: 301\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-db06726b1ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reshaped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_loss: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m             steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 10000\n  y sizes: 301\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-politics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
