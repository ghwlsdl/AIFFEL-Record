{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brazilian-mexico",
   "metadata": {},
   "source": [
    "# 23-1. 들어가며\n",
    "---\n",
    "## 목차\n",
    "1. 활성화 함수\n",
    "- 활성화 함수\n",
    "- 퍼셉트론\n",
    "    - 신경세포의 구조\n",
    "    - 퍼셉트론의 구조\n",
    "2. 선형과 비선형\n",
    "- 선형\n",
    "- 비선형\n",
    "- 비선형 함수를 쓰는 이유\n",
    "    - 그렇다면..(비선형 함수를 쓴다면?)\n",
    "3. 활성화 함수의 종류\n",
    "- 이진 계단 함수\n",
    "    -이진 계단 함수의 한계\n",
    "- 선형 활성화 함수\n",
    "    - 선형 활성화 함수의 한계\n",
    "- 비선형 활성화 함수\n",
    "- 시그모이드/로지스틱\n",
    "- 하이퍼볼릭 탄젠트\n",
    "- ReLU\n",
    "- ReLU의 단점을 극복하기 위한 시도들\n",
    "\n",
    "4. 끝으로...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-bunny",
   "metadata": {},
   "source": [
    "### 준비물\n",
    "---\n",
    "이번 노드에서 사용할 모듈 미리 import하기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-enlargement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to activate?⤴\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(7879)\n",
    "\n",
    "print('Ready to activate?⤴')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-history",
   "metadata": {},
   "source": [
    "실습에 사용할 사진에 링크를 만들어 둡시다.\n",
    "\n",
    "- mkdir -p ~/aiffel/activation\n",
    "- ln -s ~/data/* ~/aiffel/activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-values",
   "metadata": {},
   "source": [
    "# 23-2. 활성화 함수\n",
    "\n",
    "오늘은 수학 분야에서도 딥러닝과 아주아주 밀접하고 직접적인 주제를 다루어보겠습니다. 바로 softmax나 ReLU 등 이미 익숙히 들어보셨을 활성화 함수(activation function) 입니다. 🙂 **활성화 함수**란 무엇일까요? \"어떤 것이 활성화(activated)되었다\" 라는 것을 들으면 어떤 것이 떠오르시나요?\n",
    "\n",
    "넌 나의 함정 카드를 발동(활성화) 시켰다! 라는, 게임에서 종종 나오는 아래 사진과 같은 표현이 있습니다.\n",
    "\n",
    "특정 행동을 함 → 특정 조건을 만족 → 함정 카드 발동 이라는 일련의 사건을 뜻하는데요. 이처럼 활성화(activated) or 비활성화(deactivated)라는 것은 '어떤 조건을 만족 or 불만족했다'라는 것과 긴밀한 연관이 있습니다.\n",
    "\n",
    "우리가 지금까지 써왔던 신경망 속의 퍼셉트론(perceptron) 혹은 노드(node)도 '특정 조건'이 만족하면 '활성화' 되도록 디자인되어 있습니다. 노드에 입력으로 들어오는 값이 어떤 '임계치'를 넘어가면 \"활성화(activated)\"되고, 넘어가지 않으면 \"비활성화(deactivated)\"되게끔 코딩이 되어있는 것이죠. 익숙하게 써오셨던 ReLU 함수는 이러한 관점에서 어떻게 작용하고 있을까요?\n",
    "\n",
    "ReLU는 입력값이 음수라면, 즉 0 미만이라는 조건을 만족한다면 0을 출력하고, 입력값이 0이상이면 입력값 그대로를 출력하도록 디자인되어 있습니다. 이를 \"활성화 or 비활성화\"로 표현하려면 우선 활성화되는 기준을 정해야 합니다. 만일 출력값이 0보다 큰 경우를 활성화 되었다고 정의한다면, ReLU함수란, \"0 미만인 경우는 비활성화가 되고 0 이상인 경우는 활성화되는 함수\"라고 말할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-seafood",
   "metadata": {},
   "source": [
    "특정 행동을 함 → 특정 조건을 만족 → 함정 카드 발동 이라는 일련의 사건을 뜻하는데요. 이처럼 활성화(activated) or 비활성화(deactivated)라는 것은 '어떤 조건을 만족 or 불만족했다'라는 것과 긴밀한 연관이 있습니다.\n",
    "\n",
    "우리가 지금까지 써왔던 신경망 속의 퍼셉트론(perceptron) 혹은 노드(node)도 '특정 조건'이 만족하면 '활성화' 되도록 디자인되어 있습니다. 노드에 입력으로 들어오는 값이 어떤 '임계치'를 넘어가면 \"활성화(activated)\"되고, 넘어가지 않으면 \"비활성화(deactivated)\"되게끔 코딩이 되어있는 것이죠. 익숙하게 써오셨던 ReLU 함수는 이러한 관점에서 어떻게 작용하고 있을까요?\n",
    "\n",
    "ReLU는 입력값이 음수라면, 즉 0 미만이라는 조건을 만족한다면 0을 출력하고, 입력값이 0이상이면 입력값 그대로를 출력하도록 디자인되어 있습니다. 이를 \"활성화 or 비활성화\"로 표현하려면 우선 활성화되는 기준을 정해야 합니다. 만일 출력값이 0보다 큰 경우를 활성화 되었다고 정의한다면, ReLU함수란, \"0 미만인 경우는 비활성화가 되고 0 이상인 경우는 활성화되는 함수\"라고 말할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-medline",
   "metadata": {},
   "source": [
    "Q1. 활성화 함수 중엔 시그모이드라는 함수가 있습니다. 출력값이 0.5 이상이면 활성화가 된 것으로 보고, 0.5 미만이면 비활성화된 것으로 정의를 한다면, 시그모이드 함수는 입력값의 어떤 조건으로 활성화되나요?\n",
    "\n",
    "A1. 시그모이드의 입력값이 0 이상일 때 활성화가 되고, 0보다 작을 때 비활성화됩니다. 시그모이드에 대한 자세한 설명은 아래를 참고해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-latter",
   "metadata": {},
   "source": [
    "시그모이드는 ReLU처럼 명확하고 간단하게 글로 표현하긴 힘들지만, 입력값이 −∞ 로 갈수록 0을 출력하고 +∞ 로 갈수록 1을 출력하며, 0일 때는 1/2 을 출력하는 함수입니다. 수식적으로는 x를 입력값이라고 할 때, f(x)=\\dfrac{1}{1+exp(-x)}f(x)= \n",
    "1+exp(−x)\n",
    "1\n",
    "​\n",
    "  로 표현할 수 있습니다.\n",
    "이 함수는 입력값에 '무관하게' 0\\sim10∼1 사이의 값으로 출력하는 특징이 있으며, 따라서 \"참\" / \"거짓\" 혹은 \"앞면\" / \"뒷면\" 처럼 2가지 상황을 구분할 때 용이합니다. (참=0, 거짓=1 로 대응 시켜 구분할 수 있으므로)\n",
    "\n",
    "또한 활성화와 비활성화의 조건을 0.50.5로 잡았으니, 시그모이드는 00 이상일 때 활성화가 되고, 00 보다 작을 때 비활성화된다고 말할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
